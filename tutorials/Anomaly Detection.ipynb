{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Tutorial\n",
    "\n",
    "In this tutorial we will learn:\n",
    "\n",
    "- Getting Data: How to import data from PyCaret repository?\n",
    "- Setting up Environment: How to setup experiment in PyCaret to get started with building anomaly models?\n",
    "- Create Model: How to create a model and assign anomaly labels to original dataset for analysis?\n",
    "- Plot Model: How to analyze model performance using various plots?\n",
    "- Predict Model: How to assign anomaly labels to new and unseen dataset based on trained model?\n",
    "- Save / Load Model: How to save / load model for future use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger().setLevel(level=logging.ERROR)\n",
    "logging.getLogger('sintel').setLevel(level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance of the DBExplorer\n",
    "\n",
    "Sintel requires the use of MongoDB to store data.\n",
    "In order to connect to the database, all you need to do is import and create an instance of the class.\n",
    "\n",
    "To create the `DBExplorer` instance you will need to pass:\n",
    "\n",
    "* `user`: An identifier of the user that is running Orion.\n",
    "* `database`: The name of the MongoDB database to use. This is optional and defaults to `sintel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sintel.db import DBExplorer\n",
    "\n",
    "db_name = 'sintel-ad'\n",
    "dbex = DBExplorer(user='dyu', database=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will directly create a connection to the database named `'sintel-ad'` at the default\n",
    "MongoDB host, `localhost`, and port, `27017`.\n",
    "\n",
    "In case you wanted to connect to a different database, host or port, or in case user authentication\n",
    "is enabled in your MongoDB instance, you can pass a dictionary or a path to a JSON file containing\n",
    "any required additional arguments:\n",
    "\n",
    "* `host`: Hostname or IP address of the MongoDB Instance. Defaults to `'localhost'`.\n",
    "* `port`: Port to which MongoDB is listening. Defaults to `27017`.\n",
    "* `username`: username to authenticate with.\n",
    "* `password`: password to authenticate with.\n",
    "* `authentication_source`: database to authenticate against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the `DBExplorer` instance, and to be sure that we are ready to follow\n",
    "the tutorial, let's do the following two set-up steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop the currently existing `sintel-ad` database\n",
    "\n",
    "**WARNING**: This will remove all the data that exists in this database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbex.drop_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make sure to have downloaded some demo data using the `orion.data.download_demo()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.data:Downloading Sintel Demo Data to folder nasa\n"
     ]
    }
   ],
   "source": [
    "from sintel.data import download_demo\n",
    "\n",
    "download_demo(path='nasa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a folder called `orion-data` in your current directory with the 3 CSV files\n",
    "that we will use later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add information to the database\n",
    "\n",
    "\n",
    "### 1. Add a Dataset\n",
    "\n",
    "In order to add a dataset you can use the `add_dataset` method, which has the following arguments:\n",
    "\n",
    "* `name (str)`: Name of the dataset\n",
    "* `entity (str)`: Name or Id of the entity which this dataset is associated to\n",
    "\n",
    "Let's create the `Demo Dataset` that we will use for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dbex.add_dataset(\n",
    "    name='NASA',\n",
    "    entity='NASA',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call will try to create a new _Dataset_ object in the database and return it.\n",
    "\n",
    "We can now see the _Dataset_ that we just created using the `get_datasets` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>entity</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3d8a79e79c3f0225fcc</td>\n",
       "      <td>dyu</td>\n",
       "      <td>NASA</td>\n",
       "      <td>2022-02-17 02:32:55.765</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset_id created_by entity             insert_time  name\n",
       "0  620db3d8a79e79c3f0225fcc        dyu   NASA 2022-02-17 02:32:55.765  NASA"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add a Signal\n",
    "\n",
    "The next step is to add Signals. This can be done with the `add_signal` method, which expects:\n",
    "\n",
    "* `name (str)`: Name of the signal\n",
    "* `dataset (Dataset or ObjectID)`: Dataset Object or Dataset Id.\n",
    "* `start_time (int)`: (Optional) minimum timestamp to be used for this signal. If not given, it\n",
    "  defaults to the minimum timestamp found in the data.\n",
    "* `stop_time (int)`: (Optional) maximum timestamp to be used for this signal. If not given, it\n",
    "  defaults to the maximum timestamp found in the data.\n",
    "* `data_location (str)`: URI of the dataset\n",
    "* `timestamp_column (int)`: (Optional) index of the timestamp column. Defaults to 0.\n",
    "* `value_column (int)`: (Optional) index of the value column. Defaults to 1.\n",
    "\n",
    "For example, adding the `S-1` signal to the Demo Dataset that we just created could be done like\n",
    "this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal: Signal object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.add_signal(\n",
    "    name='S-1',\n",
    "    dataset=dataset,\n",
    "    data_location=os.path.join('nasa', 'S-1.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can also add all the signals that exist inside a folder by using the `add_signals`\n",
    "method, passing a `signals_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbex.add_signals(\n",
    "    dataset=dataset,\n",
    "    signals_path='nasa'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this is done, we can see that one signal has been created for each one of the CSV\n",
    "files that we downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>data_location</th>\n",
       "      <th>dataset</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3dba79e79c3f0225fcd</td>\n",
       "      <td>dyu</td>\n",
       "      <td>nasa/S-1.csv</td>\n",
       "      <td>620db3d8a79e79c3f0225fcc</td>\n",
       "      <td>2022-02-17 02:32:58.945</td>\n",
       "      <td>S-1</td>\n",
       "      <td>1222819200</td>\n",
       "      <td>1442016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620db3dba79e79c3f0225fcf</td>\n",
       "      <td>dyu</td>\n",
       "      <td>nasa/P-1.csv</td>\n",
       "      <td>620db3d8a79e79c3f0225fcc</td>\n",
       "      <td>2022-02-17 02:32:59.667</td>\n",
       "      <td>P-1</td>\n",
       "      <td>1222819200</td>\n",
       "      <td>1468540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>620db3dba79e79c3f0225fd0</td>\n",
       "      <td>dyu</td>\n",
       "      <td>nasa/E-1.csv</td>\n",
       "      <td>620db3d8a79e79c3f0225fcc</td>\n",
       "      <td>2022-02-17 02:32:59.678</td>\n",
       "      <td>E-1</td>\n",
       "      <td>1222819200</td>\n",
       "      <td>1468951200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  signal_id created_by data_location  \\\n",
       "0  620db3dba79e79c3f0225fcd        dyu  nasa/S-1.csv   \n",
       "1  620db3dba79e79c3f0225fcf        dyu  nasa/P-1.csv   \n",
       "2  620db3dba79e79c3f0225fd0        dyu  nasa/E-1.csv   \n",
       "\n",
       "                    dataset             insert_time name  start_time  \\\n",
       "0  620db3d8a79e79c3f0225fcc 2022-02-17 02:32:58.945  S-1  1222819200   \n",
       "1  620db3d8a79e79c3f0225fcc 2022-02-17 02:32:59.667  P-1  1222819200   \n",
       "2  620db3d8a79e79c3f0225fcc 2022-02-17 02:32:59.678  E-1  1222819200   \n",
       "\n",
       "    stop_time  \n",
       "0  1442016000  \n",
       "1  1468540800  \n",
       "2  1468951200  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_signals(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Template\n",
    "\n",
    "The next thing we need to add is a _Template_ to the Database using the `add_template` method.\n",
    "\n",
    "This method expects:\n",
    "\n",
    "* `name (str)`: Name of the template.\n",
    "* `template (dict or str)`: Optional. Specification of the template to use, which can be one of:\n",
    "    * An MLPipeline instance\n",
    "    * The name of a registered template\n",
    "    * a dict containing the MLPipeline details\n",
    "    * The path to a pipeline JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "template = dbex.add_template(\n",
    "    name='lstmdt',\n",
    "    template='./pipelines/orion_lstmdt.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the _Template_ that we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3e3a79e79c3f0225fd1</td>\n",
       "      <td>dyu</td>\n",
       "      <td>2022-02-17 02:33:06.667</td>\n",
       "      <td>lstmdt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                template_id created_by             insert_time    name\n",
       "0  620db3e3a79e79c3f0225fd1        dyu 2022-02-17 02:33:06.667  lstmdt"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, during this step, apart from a _Template_ object, a _Pipeline_ object has also been\n",
    "registred with the same name as the _Template_ and using the default hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>template</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3e3a79e79c3f0225fd2</td>\n",
       "      <td>dyu</td>\n",
       "      <td>2022-02-17 02:33:07.193</td>\n",
       "      <td>lstmdt</td>\n",
       "      <td>620db3e3a79e79c3f0225fd1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pipeline_id created_by             insert_time    name  \\\n",
       "0  620db3e3a79e79c3f0225fd2        dyu 2022-02-17 02:33:07.193  lstmdt   \n",
       "\n",
       "                   template  \n",
       "0  620db3e3a79e79c3f0225fd1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to use a configuration different from the default, we might want to\n",
    "create another _Pipeline_ with custom hyperparameter values.\n",
    "\n",
    "In order to do this we will need to call the `add_pipeline` method passing:\n",
    "\n",
    "* `name (str)`: Name given to this pipeline\n",
    "* `template (Template or ObjectID)`: Template or the corresponding id.\n",
    "* `hyperparameters (dict or str)`: dict containing the hyperparameter details or path to the\n",
    "  corresponding JSON file. Optional.\n",
    "\n",
    "For example, if we want to specify a different number of epochs for the LSTM primitive of the\n",
    "pipeline that we just created we will run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hyperparameters = {\n",
    "   'keras.Sequential.LSTMTimeSeriesRegressor#1': {\n",
    "       'epochs': 1,\n",
    "       'verbose': True\n",
    "   }\n",
    "}\n",
    "pipeline = dbex.add_pipeline(\n",
    "   name='lstmdt_1_epoch',\n",
    "   template=template,\n",
    "   hyperparameters=new_hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how a new _Pipeline_ was created in the Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>template</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3e3a79e79c3f0225fd2</td>\n",
       "      <td>dyu</td>\n",
       "      <td>2022-02-17 02:33:07.193</td>\n",
       "      <td>lstmdt</td>\n",
       "      <td>620db3e3a79e79c3f0225fd1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620db3e3a79e79c3f0225fd3</td>\n",
       "      <td>dyu</td>\n",
       "      <td>2022-02-17 02:33:07.793</td>\n",
       "      <td>lstmdt_1_epoch</td>\n",
       "      <td>620db3e3a79e79c3f0225fd1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pipeline_id created_by             insert_time  \\\n",
       "0  620db3e3a79e79c3f0225fd2        dyu 2022-02-17 02:33:07.193   \n",
       "1  620db3e3a79e79c3f0225fd3        dyu 2022-02-17 02:33:07.793   \n",
       "\n",
       "             name                  template  \n",
       "0          lstmdt  620db3e3a79e79c3f0225fd1  \n",
       "1  lstmdt_1_epoch  620db3e3a79e79c3f0225fd1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an Experiment\n",
    "\n",
    "Once we have a _Dataset_ with _Signals_ and a _Template_, we are ready to add an\n",
    "_Experiment_.\n",
    "\n",
    "In order to run an _Experiment_ we will need to:\n",
    "\n",
    "1. Get the _Dataset_ and the list of _Signals_ that we want to run the _Experiment_ on.\n",
    "2. Get the _Template_ which we want to use for the _Experiment_\n",
    "3. Call the `add_experiment` method passing all these with an experiment, a project name and a\n",
    "   username.\n",
    "\n",
    "For example, if we want to create an experiment using the _Dataset_, the _Signals_ and the\n",
    "_Template_ that we just created, we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dbex.add_experiment(\n",
    "    name='Demo Experiment',\n",
    "    project='Demo Project',\n",
    "    template=template,\n",
    "    dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create an _Experiment_ object in the database using the indicated _Template_\n",
    "and all the _Signals_ from the given _Dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>dataset</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>project</th>\n",
       "      <th>signals</th>\n",
       "      <th>template</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620db3e6a79e79c3f0225fd4</td>\n",
       "      <td>dyu</td>\n",
       "      <td>620db3d8a79e79c3f0225fcc</td>\n",
       "      <td>2022-02-17 02:33:10.424</td>\n",
       "      <td>Demo Experiment</td>\n",
       "      <td>Demo Project</td>\n",
       "      <td>[620db3dba79e79c3f0225fcd, 620db3dba79e79c3f02...</td>\n",
       "      <td>620db3e3a79e79c3f0225fd1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              experiment_id created_by                   dataset  \\\n",
       "0  620db3e6a79e79c3f0225fd4        dyu  620db3d8a79e79c3f0225fcc   \n",
       "\n",
       "              insert_time             name       project  \\\n",
       "0 2022-02-17 02:33:10.424  Demo Experiment  Demo Project   \n",
       "\n",
       "                                             signals                  template  \n",
       "0  [620db3dba79e79c3f0225fcd, 620db3dba79e79c3f02...  620db3e3a79e79c3f0225fd1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbex.get_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Datarun\n",
    "\n",
    "Once we have created our _Experiment_ object we are ready to start executing _Pipelines_ on our\n",
    "_Signals_.\n",
    "\n",
    "For this we will need to use the `orion.runner.start_datarun` function, which expects:\n",
    "\n",
    "* `orex (OrionExplorer)`: The `OrionDBExplorer` instance.\n",
    "* `experiment (Experiment or ObjectID)`: Experiment object or the corresponding ID.\n",
    "* `pipeline (Pipeline or ObjectID)`: Pipeline object or the corresponding ID.\n",
    "\n",
    "This will create a _Datarun_ object for this _Experiment_ and _Pipeline_ in the database,\n",
    "and then it will start creating and executing _Signalruns_, one for each _Signal_ in the _Experiment_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger a _Datarun_ using the `lstmdt_1_epoch` _Pipeline_ that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.runners.anomaly_detection:Datarun 620db3eaa79e79c3f0225fd5 started\n",
      "INFO:sintel.runners.anomaly_detection:Signalrun 620db3eaa79e79c3f0225fd6 started\n",
      "INFO:sintel.runners.anomaly_detection:Running pipeline lstmdt_1_epoch on signal S-1\n",
      "2022-02-16 21:33:18.448984: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-02-16 21:33:18.466786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fda32cdcd70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-16 21:33:18.466829: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7919 samples, validate on 1980 samples\n",
      "Epoch 1/1\n",
      "7919/7919 [==============================] - 30s 4ms/step - loss: 0.1957 - mse: 0.1957 - val_loss: 0.2921 - val_mse: 0.2921\n",
      "9899/9899 [==============================] - 11s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.runners.anomaly_detection:Processing pipeline lstmdt_1_epoch predictions on signal S-1\n",
      "INFO:sintel.runners.anomaly_detection:Signalrun 620db419a79e79c3f0225fe6 started\n",
      "INFO:sintel.runners.anomaly_detection:Running pipeline lstmdt_1_epoch on signal P-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8901 samples, validate on 2226 samples\n",
      "Epoch 1/1\n",
      "8901/8901 [==============================] - 75s 8ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.0550 - val_mse: 0.0550\n",
      "11127/11127 [==============================] - 30s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.runners.anomaly_detection:Processing pipeline lstmdt_1_epoch predictions on signal P-1\n",
      "INFO:sintel.runners.anomaly_detection:Signalrun 620db487a79e79c3f0225ff5 started\n",
      "INFO:sintel.runners.anomaly_detection:Running pipeline lstmdt_1_epoch on signal E-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8916 samples, validate on 2230 samples\n",
      "Epoch 1/1\n",
      "8916/8916 [==============================] - 49s 5ms/step - loss: 0.2340 - mse: 0.2340 - val_loss: 0.1344 - val_mse: 0.1344\n",
      "11146/11146 [==============================] - 21s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.runners.anomaly_detection:Processing pipeline lstmdt_1_epoch predictions on signal E-1\n"
     ]
    }
   ],
   "source": [
    "from sintel.runners.anomaly_detection import start_datarun\n",
    "\n",
    "start_datarun(dbex, experiment, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add anomaly detection specific information to the database \n",
    "\n",
    "The following collections will be added:\n",
    "- **signal_raw**: For each signal, save raw csv data with a given interval \n",
    "- **prediction**: For each signalrun, save the prediction results\n",
    "- **period**: For each signalrun, save the X after preprocessing in a periodical manner (year->month->day->hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sintel.db.utils:1/3: Processing signal S-1\n",
      "INFO:sintel.db.utils:2/3: Processing signal P-1\n",
      "INFO:sintel.db.utils:3/3: Processing signal E-1\n",
      "INFO:sintel.db.utils:1/3: Processing signalrun 620db3eaa79e79c3f0225fd6\n",
      "INFO:sintel.db.utils:Pipeline name lstmdt_1_epoch\n",
      "INFO:sintel.db.utils:2/3: Processing signalrun 620db419a79e79c3f0225fe6\n",
      "INFO:sintel.db.utils:Pipeline name lstmdt_1_epoch\n",
      "INFO:sintel.db.utils:3/3: Processing signalrun 620db487a79e79c3f0225ff5\n",
      "INFO:sintel.db.utils:Pipeline name lstmdt_1_epoch\n"
     ]
    }
   ],
   "source": [
    "from sintel.db.utils import update_db\n",
    "\n",
    "update_db(dbex._fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RESTful APIs to explore results\n",
    "\n",
    "Once a _Datarun_ has finished, we can see can see its status by using the `orex.get_dataruns` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sintel",
   "language": "python",
   "name": "sintel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
